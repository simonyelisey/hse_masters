{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# name of target variable\n",
    "target = 'grades'\n",
    "# list of date base features names\n",
    "date_cols = ['month', 'day', 'year', 'day_of_week', 'hour', 'time_of_day', 'season']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# original data\n",
    "train = pd.read_csv('data/train_ml.csv')\n",
    "test = pd.read_csv('data/new_test_ml.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# preprocessed data\n",
    "preprocessed_train = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "preprocessed_test = pd.read_csv(\"data/preprocessed_test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# drop nans\n",
    "\n",
    "train = train.dropna()\n",
    "\n",
    "assert train.isna().sum().sum() == 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# target\n",
    "\n",
    "y_train = train[target]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numerical features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# log the text_len and num_words features\n",
    "\n",
    "preprocessed_train[['text_len', 'num_words']] = np.log(preprocessed_train[['text_len', 'num_words']])\n",
    "preprocessed_test[['text_len', 'num_words']] = np.log(preprocessed_test[['text_len', 'num_words']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# date feature dtype -> datetime\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def extract_date_features(data: pd.DataFrame, date_col: str):\n",
    "    \"\"\"\n",
    "    The function extracts date based features from datetime feature\n",
    "\n",
    "    :param data: pd.DataFrame extract features to\n",
    "    :param date_col: str name datetime columns\n",
    "    :return: pd.DataFrame with new features\n",
    "    \"\"\"\n",
    "    data['month'] = data[date_col].dt.month\n",
    "    data['day'] = data[date_col].dt.day\n",
    "    data['year'] = data[date_col].dt.year\n",
    "    data['day_of_week'] = data[date_col].dt.day_of_week\n",
    "    data['hour'] = data[date_col].dt.hour\n",
    "    # create time of day feature\n",
    "    # 1 - 12am-05am, 2 - 06am-11am, 3 - 12pm-5pm, 4 - 6pm-11pm\n",
    "    time_of_day = [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
    "                   3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]\n",
    "    hour_to_time_of_day = dict(zip(range(0, 24), time_of_day))\n",
    "    data['time_of_day'] = data['hour'].map(hour_to_time_of_day)\n",
    "    # create season feature\n",
    "    # 1 - winter, 2 - spring, 3 - summer, 4 - autumn\n",
    "    seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n",
    "    month_to_season = dict(zip(range(1, 13), seasons))\n",
    "    data['season'] = data['month'].map(month_to_season)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# extract date features\n",
    "\n",
    "train = extract_date_features(data=train, date_col='date')\n",
    "test = extract_date_features(data=test, date_col='date')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# concat text_len & num_words with date based features\n",
    "\n",
    "train_num = pd.concat([preprocessed_train[['text_len', 'num_words']].reset_index(drop=True),\n",
    "                       train[date_cols].reset_index(drop=True)], axis=1).astype('float32')\n",
    "test_num = pd.concat([preprocessed_test[['text_len', 'num_words']].reset_index(drop=True),\n",
    "                      test[date_cols].reset_index(drop=True)], axis=1).astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# create sparse matri—Åes from numerical features\n",
    "\n",
    "train_num = scipy.sparse.csr_matrix(train_num.values)\n",
    "test_num = scipy.sparse.csr_matrix(test_num.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text data transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# tfidf transformation of text data\n",
    "\n",
    "vec = TfidfVectorizer(min_df=3, max_features=None,\n",
    "                      strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "bow_train_old = vec.fit_transform(preprocessed_train['lemmas'])\n",
    "bow_test_old = vec.transform(preprocessed_test['lemmas'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# CountVectorizer transformation of text data\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_cv = cv.fit_transform(preprocessed_train['lemmas'])\n",
    "test_cv = cv.transform(preprocessed_test['lemmas'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# CountVectorizer with ngram transformation\n",
    "\n",
    "ngram_cv = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "train_ngram_cv = ngram_cv.fit_transform(preprocessed_train['lemmas'])\n",
    "test_ngram_cv = ngram_cv.transform(preprocessed_test['lemmas'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join trasformed text data to numerical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# tf idf\n",
    "full_train_bow = scipy.sparse.hstack([bow_train_old, train_num]).tocsr()\n",
    "full_test_bow = scipy.sparse.hstack([bow_test_old, test_num]).tocsr()\n",
    "\n",
    "# countvectorizer\n",
    "full_train_cv = scipy.sparse.hstack([train_cv, train_num]).tocsr()\n",
    "full_test_cv = scipy.sparse.hstack([test_cv, test_num]).tocsr()\n",
    "\n",
    "# count vectorizer with ngram\n",
    "full_train_ngram = scipy.sparse.hstack([train_ngram_cv, train_num]).tocsr()\n",
    "full_test_ngram = scipy.sparse.hstack([test_ngram_cv, test_num]).tocsr()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "20930"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test, preprocessed_test, preprocessed_train, bow_test_old, bow_train_old, \\\n",
    "    train_cv, test_cv, train_ngram_cv, test_ngram_cv, train_num, test_num\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'n_estimators': 2000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth': 8,\n",
    "    'random_state': 42,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'feature_fraction': 0.5,\n",
    "    'num_class': 5\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 3min 16s, sys: 26min 4s, total: 5h 29min 21s\n",
      "Wall time: 50min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMClassifier(bagging_fraction=0.5, boosting_type='dart',\n               early_stopping_rounds=100, feature_fraction=0.5, max_depth=8,\n               n_estimators=2000, num_class=5, random_state=42)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "model.fit(full_train_bow, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 50min 15s, sys: 7min 36s, total: 1h 57min 51s\n",
      "Wall time: 17min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMClassifier(bagging_fraction=0.5, boosting_type='dart',\n               early_stopping_rounds=100, feature_fraction=0.5, max_depth=8,\n               n_estimators=2000, num_class=5, random_state=42)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_cv = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "model_cv.fit(full_train_cv, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 46min 32s, sys: 12min 53s, total: 3h 59min 26s\n",
      "Wall time: 35min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMClassifier(bagging_fraction=0.5, boosting_type='dart',\n               early_stopping_rounds=100, feature_fraction=0.5, max_depth=8,\n               n_estimators=2000, num_class=5, random_state=42)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_ngram = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "model_ngram.fit(full_train_ngram, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<lightgbm.basic.Booster at 0x7f84bb49b370>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the models\n",
    "\n",
    "model.booster_.save_model('models/tfidf_model.txt', num_iteration=model.best_iteration_)\n",
    "model_cv.booster_.save_model('models/cv_model.txt', num_iteration=model_cv.best_iteration_)\n",
    "model_ngram.booster_.save_model('models/cv_ngram_model.txt', num_iteration=model_ngram.best_iteration_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "p = model_ngram.predict_proba(full_test_ngram) + model_cv.predict_proba(full_test_cv) + model.predict_proba(full_test_bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# create dataframe from predictions\n",
    "\n",
    "preds = pd.DataFrame(p, columns=[1, 2, 3, 4, 5]).idxmax(axis=1).reset_index().rename(columns={'index': 'inds', 0: 'grades'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "1    12069\n5     4920\n2      162\n4       37\n3       32\nName: grades, dtype: int64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['grades'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "preds.to_csv('submission_13.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
